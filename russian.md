# screen-ocr

Эта программа предназначена для распознавания текста на скриншотах, присылаемых пользователями, особенно в Telegram например.
Поддерживается распознавание через tesseract и LLM посредством Ollama. Планируется добавить другие движки, например
easyocr и paddleocr.


## Установка

1. Для работы с Ollama Надо установить Ollama: https://github.com/ollama/ollama/blob/main/docs/linux.md

Там просто скачать архив и распаковать, можно запускать прям оттуда где распаковано, например `~/Build/ollama/bin/ollama serve` . 
Собственно эта команда должна запуститься и ждать запросов на http://localhost:11434. Оставить работать в отдельном терминале или как-то ещё.

Далее надо скачать какую-нибудь vision LLM.

Если есть GPU c 8G VRAM, то можно скачать `gemma3:4b`. Можно скачать `llama3.2-vision` - она побольше, не влезет в такую видеопамять, будет работать медленнее.
Модели можно искать на сайте ollama по слову vision например.

Сейчас в скрипте закодирована модель llama3.2-vision как дефолтная, но можно выбрать другую в конфиге или в рантайме.

То есть, для начала работы, можно скачать модель так: `~/Build/ollama/bin/ollama pull llama3.2-vision` - там 10 Гбайт или типа того, надо подождать и чтоб место было.

2. Для работы с tesseract надо установить tesseract:
```
apt install tesseract-ocr-all # На Debian
```
В конфиг-файле рекомендую установить хотя бы языки. Подробности в мануале по tesseract.

3. Рекомендую поставить терминал kitty, он нужен чтобы графический превью того что распознаётся был:
```
apt install kitty kitty-shell-integration kitty-terminfo
```
Также работа с буфером обмена нуждается во внеших коммандах, xclip или wl-paste:
```
apt install x11-apps # для X11
apt install wl-clipboard # для Wayland
```

4. Создать питоновский venv и установить туда библиотеки:
```
python3 -m venv env
source env/bin/activate
pip install -U pip setuptools  && pip install pyyaml httpx prompt_toolkit pillow pytesseract
```

5. Положить конфиг файл
```
mkdir -p ~/.config/screen-ocr
cp config.yaml ~/.config/screen-ocr/
vi ~/.config/screen-ocr/config.yaml
```

6. Можно запускать прям из терминала (kitty или другого): `python3 screenshot_ocr.py` - но можно повесить на хоткей приведённый скрипт `start_ocr` , который стартует kitty с OCR-процессом там, сначала тогда нужно установить скрипт внутрь venv:
```
pip install -e .
# можно запускать ./start_ocr
```

## Как работать

Программа берёт по умолчанию картинки типа `~/Pictures/Screenshots/Screenshot*.png` - это настраивается. Именно туда кладёт скриншоты Gnome3 под Debian или Ubuntu. То есть можно средствами Gnome выбрать область на экране, она сохранится в файл, и потом по хоткею запустить `start_ocr` . Но первой в списке идёт картинка из буфера обмена, если она там есть. Так что можно использовать любое средство взятия скриншотов, которое умеет оставлять результат в буфере обмена.

Если не изпользуется DE то можно снять скриншот внешней программой, например поставить flameshot и вызывать по хоткею (PrtScr) `flameshot gui`  или `shutter -s` .

Программа открывает последний скриншот. Moжно кнопками n и p ходить циклически по скриншотам. Кнопкой o начинается OCR. Сначала может потормозить (LLM работают медленно, tesseract работает быстрее), а потом пойдёт текст. Ctrl+C может прервать процесс. Можно выделить текст стандартным образом мышкой и скопировать. Ctrl+Shift+C - kitty копирует в буфер обмена.

В конце нажать q для выхода.

Есть выбор движка кнопкой e.
Для движка ollama есть выбор модели: кнопкой m из списка в конфиге. Или даже /model <имя> . Moжно /promt <текст> - поставить промпт.
Или кнопкой Shift+P выбрать промпт из списка (по умолчанию или в конфиге).

Есть возможность уточнить в режиме чата. После распознавания кнопой "c" открывается чат, можно ввести вопрос. 
Можно ввести несколько вопросов, но пока контекст не учитывается, только результат OCR и последний вопрос.

Выход из чата - команда "/" или Ctrl+D.
